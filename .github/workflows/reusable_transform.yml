on:
  workflow_call:
    inputs:
      ENVIRONMENT:
        required: true
        type: string
      FULL_REFRESH:
        required: true
        type: string
      DBT_CUSTOM_IMAGE:
        required: true
        type: string

jobs:
  reusable_transform:
    runs-on: ubuntu-latest
    environment: ${{ inputs.ENVIRONMENT }}
    container: ${{ inputs.DBT_CUSTOM_IMAGE }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Environment
        env:
          GOODDATA_PROFILES_FILE: "${{ secrets.GOODDATA_PROFILES_FILE }}"
        run: |
          cd ${{ vars.SRC_DATA_PIPELINE }}
          # dbt packages are installed during build of docker image to workdir
          ln -s ${{ vars.IMAGES_WORKDIR }}/dbt_packages dbt_packages
          mkdir -p ~/.gooddata
          echo ${{ env.GOODDATA_PROFILES_FILE }} | base64 --decode > ~/.gooddata/profiles.yaml

      - name: Run Transform
        timeout-minutes: 15
        env:
          FR_ARG: ${{ inputs.FULL_REFRESH == 'true' && '--full-refresh' || '' }}
          DBT_PROFILES_DIR: "${{ vars.DBT_PROFILES_DIR }}"
          # dbt cloud insist on env variables must contain DBT_ prefix. We have to duplicate them here.
          # dbt profiles.yml file in this repo relies on DBT_ prefix.
          # It means that even jobs not running against dbt cloud rely on DBT_ prefix.
          # More variables are duplicated later in this file based on what database is used.
          DBT_OUTPUT_SCHEMA: "${{ vars.OUTPUT_SCHEMA }}"
          DBT_INPUT_SCHEMA_GITHUB: "${{ vars.INPUT_SCHEMA_GITHUB }}"
          DBT_INPUT_SCHEMA_FAA: "${{ vars.INPUT_SCHEMA_FAA }}"
          DBT_INPUT_SCHEMA_EXCHANGERATEHOST: "${{ vars.INPUT_SCHEMA_EXCHANGERATEHOST }}"
          DBT_INPUT_SCHEMA_ECOMMERCE_DEMO: "${{ vars.INPUT_SCHEMA_ECOMMERCE_DEMO }}"
          DBT_INPUT_SCHEMA_DATA_SCIENCE: "${{ vars.INPUT_SCHEMA_DATA_SCIENCE }}"
          DBT_DB_USER: "${{ vars.DB_USER }}"
          # TODO: no need to generalize WAREHOUSE/ACCOUNT, they are Snowflake specific
          DBT_DB_WAREHOUSE: "${{ vars.SNOWFLAKE_WAREHOUSE }}"
          DBT_DB_ACCOUNT: "${{ vars.SNOWFLAKE_ACCOUNT }}"
          DBT_DB_HOST: "${{ vars.DB_HOST }}"
          DBT_DB_PORT: "${{ vars.DB_PORT }}"
          DBT_DB_NAME: "${{ vars.DB_NAME }}"
          DBT_DB_PASS: "${{ secrets.DB_PASS }}"
          # TODO - move it to separate job dedicated to dbt Cloud
          # Notify by sending comment to the merge request,
          # if duration of a dbt model exceeds average duration from last X runs by DBT_ALLOWED_DEGRADATION percents
          DBT_ALLOWED_DEGRADATION: 20
          DBT_INCREMENTAL_STRATEGY: "merge"
          GOODDATA_PROFILES: "${{ vars.GOODDATA_PROFILES }}"
        run: |
          cd ${{ vars.SRC_DATA_PIPELINE }}
          dbt run --profiles-dir ${{ vars.DBT_PROFILES_DIR }} --profile ${{ vars.ELT_ENVIRONMENT }} --target ${{ vars.DBT_TARGET }} ${{ env.FR_ARG }}
          dbt test --profiles-dir ${{ vars.DBT_PROFILES_DIR }} --profile ${{ vars.ELT_ENVIRONMENT }} --target ${{ vars.DBT_TARGET }}
          gooddata-dbt provision_workspaces
          gooddata-dbt register_data_sources ${{ vars.GOODDATA_UPPER_CASE }} --profile ${{ vars.ELT_ENVIRONMENT }} --target ${{ vars.DBT_TARGET }}
          gooddata-dbt deploy_ldm ${{ vars.GOODDATA_UPPER_CASE }} --profile ${{ vars.ELT_ENVIRONMENT }} --target ${{ vars.DBT_TARGET }}
          # Invalidates GoodData caches
          gooddata-dbt upload_notification --profile ${{ vars.ELT_ENVIRONMENT }} --target ${{ vars.DBT_TARGET }}
